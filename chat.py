import streamlit as st
from dotenv import load_dotenv
import os
import pandas as pd
from langchain_groq import ChatGroq  # Biblioteca para usar Groq com LangChain
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.memory import ConversationBufferMemory
from utils.normalize_dataframe import normalize_date

# Carregar vari√°veis de ambiente
load_dotenv()
os.environ['GROQ_API_KEY'] = os.getenv('TOKEN_GROQ')  # Substituir pelo token correto

# Classe do agente de intera√ß√£o
class AgentTalkingCarga:
    def __init__(self, model, dataframe):
        self.dataframe = dataframe
        self.llm = ChatGroq(model=model, api_key=os.environ['GROQ_API_KEY'], streaming=True)  # Habilitando streaming
        self.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        self.create_agent()

    def create_agent(self):
        agent_prompt_prefix = """
            Voc√™ √© um assistente amig√°vel especializado em fornecer informa√ß√µes estruturadas e precisas.
            [Instru√ß√µes detalhadas de resposta, iguais ao exemplo anterior]
        """
        df = normalize_date(self.dataframe)
        self.agent = create_pandas_dataframe_agent(
            self.llm,
            df,
            prefix=agent_prompt_prefix,
            verbose=True,
            agent_type='tool-calling',  # Ajuste para usar ferramentas da Groq
            allow_dangerous_code=True
        )

    def stream_response(self, user_input):
        """Gera a resposta em streaming com valida√ß√£o de chunk."""
        try:
            for chunk in self.agent.stream(user_input):
                if isinstance(chunk, dict) and "output" in chunk:
                    yield chunk["output"]
                else:
                    print(f"Formato inesperado do chunk: {chunk}")  # Debugging
        except Exception as e:
            print(f"Erro no streaming: {e}")  # Debugging
            raise


# Fun√ß√£o de interface do Streamlit
def chatbot_interface():
    # Criando o dataframe fict√≠cio (ou substitua por seu pr√≥prio)
    df = pd.read_excel('atracacao.xlsx')
    
    # Configura√ß√£o inicial
    st.set_page_config(page_title='Agent Port', page_icon='üö¢')
    
    # Sidebar
    st.sidebar.title("Configura√ß√µes do Chatbot")
    models = ["llama-3.1-70b-versatile",
              "llama-3.2-90b-vision-preview",
              "gemma2-9b-it"]
    
    model_choice = st.sidebar.selectbox("Escolha o modelo", models, key="model_choice")

    # Inicializando o agente com o modelo e dataframe
    agent = AgentTalkingCarga(model_choice, df)

    # Central
    st.header('ü§ñ Agent Port üö¢')

    # Hist√≥rico da conversa
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Exibir mensagens na tela
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # Entrada do usu√°rio
    user_input = st.chat_input("Digite sua pergunta:")
    if user_input:
        # Adicionar pergunta do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").markdown(user_input)

        # Preparar para exibir a resposta em streaming
        response_container = st.chat_message("assistant")
        response_text = ""

        # Exibir o spinner enquanto processa a resposta
        with st.spinner('Buscando informa√ß√µes. Aguarde!'):
            try:
                for chunk in agent.stream_response(user_input):
                    response_text += chunk
                    response_container.markdown(response_text)
            except Exception as e:
                st.error(f"Erro ao processar resposta: {e}")
        
        # Adicionar a resposta completa ao hist√≥rico
        st.session_state.messages.append({"role": "assistant", "content": response_text})


# Executar a interface do Streamlit
if __name__ == "__main__":
    chatbot_interface()
